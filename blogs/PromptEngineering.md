---
title: 深入了解提示工程
date: 2024-4-17 10:41:36
tags: [AI,学习笔记]
category: AI
---

### 什么是提示工程？

提示工程(Prompt Engineering)用于提升大语言模型(Large Language Model, LLM)在处理复杂任务场景的能力，比如问答或推理计算能力。这是一门比较新的学科，随着AI应用的爆发性增长而进入大众视野。

提示工程在与大模型交互，对接，以及理解大模型预言能力等方面发挥着重要作用，我们熟知的ChatGPT就使用了提示工程来提高多轮对话的效果。

比较现实的是，工程师个人难以拥有深度学习所需的算力，无法提供模型训练和微调所需的资源，在某些特定场景下提示工程可能是门槛较低的优化大模型方案之一。

提示词由四个要素组成：

**指令：**想要模型执行的任务

**上下文：**给模型提供一些额外的上下文信息，用于引导模型输出更优质的内容

**输入数据：**用户输入的内容或问题

**输出指示：**指定模型输出的类型或格式

> 展示一段代码
>
> 要求输出"hello word"
>
> 编程语言为Go语言

大模型输出:

```go
以下是一个简单的 Go 代码段，用于输出 “hello world”：
package main

import "fmt"

func main() {
    fmt.Println("hello world")
}
```

上例示例中，指令或任务是"展示一段代码"，输出数据为"要求输出'hello word'",输出指示为"编程语言为Go语言"。上例示例中并没有包含**上下文**，但是大模型依然输出了我们期望的内容，可见提示词并非上述元素都是必须的。同时通过大模型的输出，我们可以发现"要求输出'hello word'",被大模型输出为`fmt.Println("hello world")`,模型矫正了单词错误。

在使用提示词时，一般工程师会通过API调用的方式或直接与大模型进行交互，在这个阶段可以通过配置一些参数用来优化输出结果。不同参数对于输出的结果非常重要，常见的一些参数设置：

**Temperature**:这个参数值越小，模型返回的结果越精确，这个参数越大，模型返回的随机性越大。在一些需要精确结果的应用场景下可以将这个值调小，反之在一些创造性强的场景下，例如生成诗歌，编一个故事…可以适当通过调高参数值以丰富模型输出。

**Max Length**: 这个参数用于控制大模型Token输出，可以避免大模型生成冗长或无意义的响应。

**Stop Sequences：**用来控制大模型输出的另一种方式，是一个string类型，设置可以控制列表输出项等。

**Frequency Penalty:** 用来调整多轮对话中，某个词在模型输出中出现的次数，该参数越高，某个词再次出现的可能性就越小。

上例参数是大模型可调参数的一部分，具体可根据大模型[相关文档](https://open.bigmodel.cn/dev/api#glm-4)进行阅读，本文所有示例基于GLM-4，但不确保模型响应与本文示例保持一致。

在OpenAI官网提供的提示工程指南中提出了六个原则：

- 写出清晰的指令
- 提供参考文本
- 将复杂的任务拆分成简单的子任务
- 给模型'思考'的时间
- 使用外部工具
- 系统地测试变更



### 零样本提示和少样本提示

经过大量训练出来的大模型已经能够执行零样本任务，即不提供任何示例，模型依然会输出相对符合期望的结果，这源于RLHF技术在深度学习时让模型更好适应人类偏好。但在更复杂的场景或问题下，零样本下的大模型无法输出更优质的结果，这便衍生出少量样本提示来引导模型。

零样本提示：

```tex
输入："请用'槑雷'造句"

输出：我今天在网吧玩游戏，结果连续遇到槑雷的队友，真是让人哭笑不得。
```

少量样本提示：

```tex
输入: '槑雷'是一种生活用品，一个使用'槑雷'这个词的句子的例子是：在洗碗的时候使用槑雷，能让碗变得非常干净"

	   '雷槑'指一段非常好的代码，请用'雷槑'造句

输出:  编写程序时，我用了一段雷槑的代码，让整个程序运行效率大幅提升，性能检测结果让人非常满意。
```



我们通过上述例子可以观察到，'槑雷'这个人为臆造的词在零样本生成下可能词不达意，但是在少样本提示下，大模型通过示例已经学会了如何正确使用该词。

标签和格式也会对大模型的输出起作用：

```
输入： 你代码写的太好啦！// 夸赞
      你代码写的太差了！// 批评
      你的裙子真不错！//
      
输出：夸赞
```

即使是随机标签也会对输出有帮助，毕竟有总比没有好。

少量样本提示同时也会存在限制，

```
输入：这组数字中的奇数加起来是一个偶数：151、321、51、131、821、71、121。
输出：让我们将给定的奇数相加，并检查其和是否为偶数。
     151 + 321 + 51 + 131 + 821 + 71 + 121 = 1766
     现在，我们来确定这个结果是否为偶数。一个数如果可以被2整除，那么它就是偶数。在这种情况下，1766可以被2整除，因为其个位数是6（偶数的个位数总是0, 2, 4, 6, 或8）。
     因此，这些奇数相加的和1766确实是一个偶数。这符合数学规律，即两个奇数相加总是得到一个偶数（因为两个奇数相加，其个位数总是两个奇数个位数相加等于偶数的结果，从而使得和为偶数），所以多个奇数相加的和也有可能是偶数。在这个例子中，七个奇数相加得到了一个偶数。
```

可以拿计算器试一下，大模型这方面是反直觉的，可能认为基础的计算是百无一失的，实际上`151 + 321 + 51 + 131 + 821 + 71 + 121`并不等于 `1766`而是 `1677`，七个奇数相加怎么会得出偶数结果呢？这个结果与大模型后续输出的内容也是相悖的。

即使提供少样本提示也很难获得这种类型的推理问题的可靠响应。



### 思维链提示

当任务过于复杂，少样本提示无法起作用时。可以使用思维链(CoT)提示，还记得最开始OpenAI推荐的六个原则吗？将复杂的任务拆解成简单的子任务。就像人类在面对上述计算题会将逐步数字逐步拆分，在将多个结果整合后给出最终答案，但是有一个更简单的方式：

```
输入：151+321+51+131+121+71+821 结果是多少？是奇数还是偶数？让我们逐步思考
输出：我们可以把所有的数相加：151 + 321 + 51 + 131 + 121 + 71 + 821，最终结果是1667。
     1667不是2的倍数，因此它是一个奇数。

```

"让我们逐步思考"提示是一种自动化思维链展示，在没有太多的示例提供时它相当好用。假设你使用人工手动解析这一计算过程，也可能会输入错误信息(毕竟人类也会犯错，当然，现在大模型也会)而误导大模型。这种自动思维链的方式可以消除人工的方法，当然这种自动的过程也会出错，所以演示的多样性依旧重要：

Auto-CoT 两个阶段：

1.问题聚类：给问题划分为几个聚类

2.演示抽样，从每组数据中选出一个具有代表性的问题，并使用带有简单启发式的 Zero-Shot-CoT 生成其推理链。

简单的启发式方法可以是问题的长度（例如，60 个 tokens）和理由的步骤数（例如，5 个推理步骤）。这鼓励模型使用简单而准确的演示。具体可见https://github.com/amazon-science/auto-cot
