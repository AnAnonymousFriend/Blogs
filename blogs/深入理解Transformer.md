---
title: 深入理解Transformer自注意力机制
date: 2024-5-20 12:49:00
tags: [LLM,Transformer]
category: AI
---

### 前言

2017年，谷歌团队推出一篇神经网络的论文，首次提出将"自注意力"机制引入深度学习，这一机制可以按输入数据各部分重要性的不同而分配不同的权重。当ChatGPT震惊世人时，Transformer也随之进入大众视野。一夜之间，AI创业公司层出不穷，掌握算力的互联网寡头们争相推出自己的大语言模型，这些模型都基于Transformer神经网络架构，比如ChatGPT只使用了其中的解码器，DeBERTa只使用了其编码器，Flan-UL2则编码解码全都使用。

而对于用户来说，大语言模型还是一个黑盒，用户只知道输入一些简单指令模型便会产生一些输出，这些输出可能满足用户的需求，也有可能不满足，于是用户通调整指令的方式来得到不同输出的结果。从笼统，抽象的概括到指令精确的下发，这也推进了提示词工程的发展。很难评价是机器在学习人类还是人类在适应机器，亦或都有。

开发者的世界中可能了解得更多，比如使用LangChain或LlamaIndex构建RAG(检索增强生成)系统，使用提示词工程优化输出结果，设置`temperature`等各类参数控制大模型创新性等……虽然比用户更接近黑盒，但依然存有很多无法解答的问题：为什么大语言模型会有上下文的限制？为什么现阶段的模型没有长期记忆？为什么要使用Transformer作为基础？

对于科学家来说上述问题可能很好回答，但大部分表述都是一些晦涩难懂的专有名词，谈论得更多的是向量，前馈神经网络，嵌入层，层归一化，矩阵……以及一堆复杂的数学公式。在学术界热热闹闹讨论AGI未来的同时，这些专属名词构建出的壁垒让普通人难以望其颈背，无疑于在黑盒之上又蒙了一层神秘的面纱。

![image-20240523233355443](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240523233355443.png)

这当然不是科学家有意为之，但从也从侧面反映人工智能领域的复杂性，一个包含计算机科学，数学，认知科学，心理学等众多学科的领域要解释起来确实颇有难度。

本篇文章目的意在使用通俗语言解释这些专有名词，并通过数据流向的方式描绘“自注意力机制”在训练时的过程，但不会事无巨细地解释其中每一个细节。



### 向量

当我们谈论向量的时候，我们具体谈论的是哪一领域的‘向量’？是数学，物理学还是其他自然科学中的基本概念？

是理论数学中的定义：任何在称为[向量空间](https://zh.wikipedia.org/wiki/向量空间)的代数结构中的元素。一般地，同时满足具有大小和方向两个性质的几何对象即可认为是向量。

还是物理领域中的定义：向量是空间中的箭头，它指向三维空间其中一点，以三元数组代表某个特定的对象，这个对象可以代表任意东西。

无论使用哪一领域的向量定义，最终都要回到一个根本性问题，如何让机器识别某一信息代表的具体意义？

对于机器而言，人类世界的语言并没有什么意义，无论是使用中文的'他'，还是英文中的'he'，机器能识别和处理只能是数值，例如0,1。为了让机器更好识别人类语言及真实世界，科学家们使用向量来表示某一词或字。一方面向量可以将文本转为数据，使得机器可以对文本进行数学运算；另一方面向量可以包含多种特征，如词性，上下文等，这有助于提高模型对语言的理解能力。

假设使用一个二维坐标轴将人类语言词汇分布在上面，英文的'he'和中文的'他'可能会分布在不同轴上，但人类世界的复杂性可能远远超出二维空间代表的含义，例如“番茄”和“西红柿”只是两种不同叫法，指向的是同一种植物果实，却需要要占据两个向量(即使它们可能无比接近)。

如果将一个二维空间升维成三维空间，'番茄'这一含义代表着三维空间的某坐标，意思相近的词依旧分布在其附近，但不同维度的空间承载的向量则发生指数级的变化。

事实上向量可以是任意维度的，这取决于数据的复杂性和所需的特征数量。高维向量可以捕捉更多的细节和信息，但也导致了计算的复杂性。

![image-20240524142225880](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240524142225880.png)

模型将输入内容称为`token`，在文本生成型模型中这些token往往代表单词或片段或其他常见的字符组合；但对于分析音频大模型来说，token可能代表声音片段；分析图片大模型来说，token可能代表小块图片。



### 嵌入矩阵









### 学习资料

《ChatGPT原理与架构》

[GPT是什么？直观解释Transformer](https://www.bilibili.com/video/BV13z421U7cs?vd_source=c35ebf3f8a69ea3c94ec04093aaae458)

[直观解释注意力机制，Transformer的核心](https://www.bilibili.com/video/BV1TZ421j7Ke?vd_source=c35ebf3f8a69ea3c94ec04093aaae458)

https://transformers.run/c1/transformer/





