---
title: 深入理解Transformer自注意力机制
date: 2024-5-20 12:49:00
tags: [LLM,Transformer]
category: AI
---

### 前言

2017年，谷歌团队推出一篇神经网络的论文，首次提出将"自注意力"机制引入深度学习中，这一机制可以按输入数据各部分重要性的不同而分配不同的权重。当ChatGPT震惊世人时，Transformer也随之进入大众视野。一夜之间，AI创业公司层出不穷，掌握算力的互联网寡头们争相推出自己的大语言模型，这些模型都基于Transformer神经网络架构，比如ChatGPT只使用了其中的解码器，DeBERTa只使用了其编码器，Flan-UL2则编码解码全都使用。

而对于用户来说，大语言模型还是一个黑盒，用户只知道输入一些简单指令模型便会产生一些输出，这些输出可能满足用户的需求，也有可能不满足，于是用户通调整指令的方式来得到不同输出的结果。从笼统，抽象的概括到指令精确的下发，这也推进了提示词工程的发展。很难评价是机器在学习人类还是人类在适应机器，亦或都有。

开发者的世界中可能了解得更多，比如使用LangChain或LlamaIndex构建RAG(检索增强生成)系统，使用提示词工程优化输出结果，设置`temperature`等各类参数控制大模型创新性等……虽然比用户更接近黑盒，但依然存有很多无法解答的问题：为什么大语言模型会有上下文的限制？为什么现阶段的模型没有长期记忆？为什么要使用Transformer作为基础？

对于科学家来说上述问题可能很好回答，但大部分表述都是一些晦涩难懂的专有名词，谈论得更多的是向量，前馈神经网络，嵌入层，层归一化，矩阵乘积……以及一堆复杂的数学公式。在学术界热热闹闹讨论AGI未来的同时，这些专属名词构建出的壁垒让普通人难以望其颈背，无疑于在黑盒之上又蒙了一层神秘的面纱。

<img src="https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240523233355443.png" alt="image-20240523233355443"  />

这当然不是科学家有意为之，但从也从侧面反映人工智能领域的复杂性，一个包含计算机科学，数学，认知科学，心理学等众多学科的领域要解释起来确实颇有难度。

本篇文章目的意在使用通俗语言解释这些专有名词，并通过数据流向的方式描绘“自注意力机制”在训练时的过程，但不会事无巨细地解释其中每一个细节。



### 向量

当我们谈论向量的时候，我们具体谈论的是哪一领域的‘向量’？是数学，物理学还是其他自然科学中的基本概念？

是理论数学中的定义：任何在称为[向量空间](https://zh.wikipedia.org/wiki/向量空间)的代数结构中的元素，并同时满足具有大小和方向两个性质的几何对象即可认为是向量。

还是物理领域中的定义：向量是空间中的箭头，它指向三维空间其中一点，以三元数组代表某个特定的对象，这个对象可以代表任意东西。

无论使用哪一领域的向量定义，最终都要回到一个根本性问题，如何让机器识别某一信息代表的具体意义？

对于机器而言，人类世界的语言并没有什么意义，无论是使用中文的'他'，还是英文中的'he'，机器能识别和处理只能是数值，例如0,1。为了让机器更好识别人类语言及真实世界，科学家们使用向量来表示某一词或字。向量可以将文本转为数据，使得机器可以对文本进行数学运算；向量也可以包含多种特征，如词性，上下文等，这有助于提高模型对语言的理解能力。

假设使用一个二维坐标轴将人类语言词汇分布在上面，英文的'he'和中文的'他'可能会分布在不同轴上。“番茄”和“西红柿”只是两种不同叫法，指向的是同一种植物果实，却需要要占据两个向量，即使它们可能无比接近。

如果将一个二维空间升维成三维空间，'番茄'这一含义代表着三维空间的某个坐标(x,y,z)，意义相近的词依旧分布在其附近。看似好像没有什么区别，但描述同一对象的方式却发生了改变。事实上向量可以是任意维度的，这取决于数据的复杂性和所需的特征数量，不同维度承载的特征数量是不一样的。通俗一点讲就是：向量的维度越高，代表的含义也就越多。高维向量可以捕捉更多的细节和信息，但这也导致了计算的复杂性。

<img src="https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240524142225880.png" alt="image-20240524142225880" style="zoom: 50%;" />

向量是一切的基础，无论是模型训练还是使用AI应用，第一步都是将输入的文本/图片/音频等信息转换为向量，一般情况下我们将这些输入/输出的向量统称`token`。为了方便理解，我们暂时将一个单词对应一个向量，即一组数字。

<img src="https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240527154443342.png" alt="image-20240527154443342" style="zoom:67%;" />

### 嵌入矩阵

通常情况下模型会包含一个预设的词汇库，它被称为"嵌入矩阵"。以ChatGPT-3为例，这个嵌入矩阵中包含了50257个token，12288个维度，里面可以是单词或标点符号……它们的初始值随机，但将基于数据进行学习。这是故意为之，如果所有嵌入向量都初始化为相同的值，那么模型在训练时无法区分不同的输入。使用随机值可以打破均匀性，避免训练开始时的梯度相似，同时也可以避免模型陷入局部最优解。

<img src="https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240527231132420.png" alt="image-20240527231132420" style="zoom: 33%;" />

早在Transformer出现前，将单词转化为向量已经是机器学习中常见的做法，到如今Transformer几乎成为自然语言处理，视觉处理，多模态处理的基础模型。可见将人类所有语言词汇转变为向量并不能一蹴而就，这就像拥有一座铁矿并不意味着得到好钢，但它为接下来的淬炼奠定了基础。

这非常重要，举一个[3Blue1Brown](https://www.3blue1brown.com/)提出的小例子：当模型在训练阶段调整权重时，以确定不同单词将如何被嵌入向量，它们最终的嵌入向量在空间中的方向，往往具备某种语义意义。

就像在谈论向量时那样描述的，意义相近的向量会分布的较为接近。如果取[女人]和[男人]的向量之差，从一个向量的尖端指向另一个尖端，便会发现这个向量差与[父亲]和[母亲]的向量差非常相似。假设你并不知道[母亲]这一词在向量空间中的分布，但是可以通过[父亲]这一词汇加上[女人]减去[男人]的方向，然后搜寻最接近该点的词向量来找到它。

<img src="https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240528105204253.png" alt="image-20240528105204253" style="zoom:50%;" />

上述例子足以说明，**空间中的方向能够承载语义**。反复强调它的重要性，是因为嵌入矩阵中的向量不能仅仅将其视为单个单词，它编码了单词的位置信息，还结合了上下文语境。在现实世界，不同的语境下同一词也代表不同的含义，“model”这一词是指艺术领域中展示服装的人类，是指计算机领域的数据结构？还是我们现在谈论的机器学习模型？

假如对模型提出一个问题：“the greatest thinker in Chain is ?”

`greatest`这个词向量会被网络中各个模块拉扯，最终指向一个更具象的方向或对象。如下图中所示，上下文中的Chain指明了向量被拉扯的方向，最终指向的方向范围可能会包含"孔子"这一具体的向量词。

![image-20240528112649751](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240528112649751.png)

一个训练得好的注意力模块目的就是：**能计算出给初始的词向量加入什么样的向量，才能将它移动到上下文对应的具体方向上。**



### 自注意力机制

本文开篇中提到过注意力机制**可以按输入数据各部分重要性的不同而分配不同的权重**，但在深度学习中模型的实际行为很难解释，且其多数计算都是矩阵向量乘法，注意力机制在矩阵中填满了可调的权重，这些权重由模型学习数据来进行调整，最终计算点积和层归一化后得到输出的token。

自注意力的本质：
$$
AT(Q,K,V) = softmax(XX^T)X
$$

如果本文剩余篇幅只是例举公式，那么似乎很难让人理解，也违背了本文最初的承诺。这就像把向量矩阵呈现在读者面前，让读者自己计算token一样荒诞。作者将沿用上述例举的方式，尽可能避开公式的细节，抽象出机制的行为并绘制整个流程，可能尽管没有公式那么精确，但容易帮助我们了解到底发生了什么。

在训练之初，输出文本创建向量组时，每个向量都是直接从嵌入矩阵中提取出来的。

![截屏2024-05-29 22.38.51](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/%E6%88%AA%E5%B1%8F2024-05-29%2022.38.51.png)

然后这个向量组会流向三个向量矩阵：

1.Query 查询矩阵的维度比嵌入向量小得多，要计算查询向量需要先初始化一个随机向量组并记做 Wq ，然后在乘以嵌入向量。从数学的角度来看它们只是在对向量矩阵做乘积，但抽象成一个具像化的行为，更像是对输入的单词提出一系列问题。

<img src="https://raw.githubusercontent.com/AnAnonymousFriend/images/main/%E6%88%AA%E5%B1%8F2024-05-29%2023.34.08.png" alt="截屏2024-05-29 23.34.08" style="zoom: 33%;" />

2.Key 键矩阵与查询矩阵相同，但从概念上来讲，可以把[键矩阵]想象成想要回答[查询]，当键与查询的方向对齐时，就能认为它们相匹配。



在训练时，根据输入文本创建向量组时，每个向量都是直接从嵌入矩阵中提取出来的。当这些向量流入一个矩阵网络后，这些向量获得比单个词更丰富更具体的含义。这种网络一次只能处理特定数量的向量，称作它的上下文长度，ChatGPT-3的上下文长度为2048，因此流经网络的数据有2048列，12288个维度。上下文长度限制了Transformer在预测下一个词时能结合的文本量。

这也解释了为什么ChatGPT没有长期记忆，在长对话时会有一种"健忘"的感觉。





### Softmax函数

让我们略过自注意力机制，将视线集中到模型最终输出的结果上，我们沿用上述例子：“the greatest thinker in China is ？”



























### 学习资料

《ChatGPT原理与架构》

[GPT是什么？直观解释Transformer](https://www.bilibili.com/video/BV13z421U7cs?vd_source=c35ebf3f8a69ea3c94ec04093aaae458)

[直观解释注意力机制，Transformer的核心](https://www.bilibili.com/video/BV1TZ421j7Ke?vd_source=c35ebf3f8a69ea3c94ec04093aaae458)

https://transformers.run/c1/transformer/





