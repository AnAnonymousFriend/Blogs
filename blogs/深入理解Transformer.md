---
title: 深入理解Transformer自注意力机制
date: 2024-5-20 12:49:00
tags: [LLM,Transformer]
category: AI
---

### 前言

2017年，谷歌团队推出一篇神经网络的论文，首次提出将"自注意力"机制引入深度学习，这一机制可以按输入数据各部分重要性的不同而分配不同的权重。当ChatGPT震惊世人时，Transformer也随之进入大众视野。一夜之间，AI创业公司层出不穷，掌握算力的互联网寡头们争相推出自己的语言模型，这些模型都基于Transformer神经网络架构，比如ChatGPT只使用了其中的解码器，DeBERTa只使用了其编码器，Flan-UL2则编码解码全都使用。

而对于用户来说，大语言模型还是一个黑盒，用户只知道输入一些简单指令模型便会产生一些输出，这些输出可能满足用户的需求，也有可能不满足，于是用户通调整指令的方式来得到不同输出的结果，从笼统，抽象的概况到指令精确的下发，这也推进了提示词工程的发展。很难评论是机器在学习人类还是人类在适应机器，亦或都有。

开发者的世界中可能了解得更多，比如使用LangChain或LlamaIndex构建RAG(检索增强生成)系统，使用提示词工程优化输出结果，设置`temperature`等各类参数控制大模型创新性等……虽然比用户更接近黑盒，但依然存有很多无法解答的问题：为什么大语言模型会有上下文的限制？为什么现阶段的模型没有长期记忆？为什么要使用Transformer作为基础？

对于科学家来说上述问题可能很好回答，但大部分表述都是一些晦涩难懂的专有名词，谈论得更多的是向量化，嵌入层，层归一化，矩阵等。本篇文章目的意在使用通俗语言解释这些专有名词，并通过数据流向的方式描绘“自注意力机制”在训练时的过程。

![image-20240523233355443](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240523233355443.png)

### 向量

在数学领域中，向量代表着任何称为向量空间的代数结构中的元素，一般来说同时满足具有大小和方向两个性质的几何对象即可认为是向量。如果使用一个二维的坐标轴将所有人类语言词汇分布在上面，含义相同的词会分布得较为接近，例如下图中的“番茄”和“西红柿”。

物理领域中，向量是空间中的箭头，它指向三维空间其中一点，以[3,4,2]这样三元数组代表某个特定的对象，这个对象可以代表任意东西。

到了计算机领域，对于机器来说人类世界的语言并没有太大的意义，机器只能识别数字，为了让机器更好识别人类语言及真实世界，科学家们使用向量来表示某一词或字。这种抽象具体化的方式一方面更贴近机器的习惯，一方面也更符合数学计算。

![image-20240524142225880](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20240524142225880.png)











### 学习资料

《ChatGPT原理与架构》

[GPT是什么？直观解释Transformer](https://www.bilibili.com/video/BV13z421U7cs?vd_source=c35ebf3f8a69ea3c94ec04093aaae458)

[直观解释注意力机制，Transformer的核心](https://www.bilibili.com/video/BV1TZ421j7Ke?vd_source=c35ebf3f8a69ea3c94ec04093aaae458)

https://transformers.run/c1/transformer/





