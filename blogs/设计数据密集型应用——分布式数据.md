---
title: 《设计数据密集形应用》第二章读书笔记
date: 2023-8-15 16:47:00
tags: [分布式,学习笔记]
category: 分布式

---

# 分布式数据

《设计数据密集型应用》一书第二部分，分布式数据中涉及到了数据的复制,如何分区，分布式事务，讨论了分布式系统的麻烦及解决方案，一致性与共识的认知。



## 复制

复制代表着通过网络连接的多台机器上保留着相同的副本。一般情况下使用复制功能，我们更期望的是：

1.让用户在地理上更接近数据(从而减少延迟)

2.系统的一部分出现故障，系统也能继续工作(提高可用性)

3.伸缩可以接受读请求的机器数量(提高吞吐量)

如果复制的数据不会随着时间而改变，那复制就变得很简单：将数据复制到每节点，仅需一次就好了。但复制的困难往往出现在处理复制数据的**变更**。

本小结将会讨论三种流行的变更复制算法：**单领导者（single leader，单主）**，**多领导者（multi leader，多主）** 和 **无领导者（leaderless，无主）**。几乎所有分布式数据库都使用这三种方法之一。

当然，在复制时也同样进行很多权衡，使用同步复制还是异步复制？如何处理失败的副本？



### 领导者与追随者

存储了数据拷贝的每个节点被称为**副本(replica)**。当多个副本存在时就有出现一个问题：如何确保所有数据都存在副本上？

每一次向数据库的写入操作都需要传播到所有副本上，否则副本数据就不能保持一致。

**基于领导者的复制（leader-based replication）** （也称 **主动/被动（active/passive）** 复制或 **主/从（master/slave）** 复制）

1.在多个副本中选一个副本指定其为**领导者(leader)**,有时也被称为**主库(master|primary)**。当客户端发送写入请求时，它必须将请求发送给**领导者**，其会将新数据写入其本地存储。

2.其他副本被称为 **追随者（followers）**，亦称为 **只读副本（read replicas）**、**从库（slaves）**、**备库（ secondaries）** 或 **热备（hot-standby）**[1](https://github.com/Vonng/ddia/blob/master/ch5.md#user-content-fn-i-8006e2d5a73957a25f1adcd45244d11b)。每当领导者将新数据写入本地存储时，它也会将数据变更发送给所有的追随者，称之为 **复制日志（replication log）** 或 **变更流（change stream）**。每个跟随者从领导者拉取日志，并相应更新其本地数据库副本，方法是按照与领导者相同的处理顺序来进行所有写入。

3.当客户想要从数据库中读取数据时，它可以向领导者或任一追随者进行查询。但只有领导者才能接受写入操作（从客户端的角度来看从库都是只读的）。

![image-20230815174416749](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20230815174416749.png)

#### 同步复制和异步复制

复制系统的一个重要细节是：复制是 **同步（synchronously）** 发生的还是 **异步（asynchronously）** 发生的。

通常情况下复制的速度相当快：大多数数据库系统能在一秒内完成从库的同步，但它们不能提供复制用时的保证。在某些情况下从库可能落后主库几分钟或者更久：比如从库正在从故障中恢复，系统正在最大容量附近运行，或者当节点间存在网络问题时。

同步复制的优点：从库能保证与主库一致的最新数据副本。如果主库突然失效，我们依然能在从库上找到这些数据。

同步复制的缺点：如果同步从库没有响应(比如从库已经崩溃，或者网络出现故障)，主库就无法处理写入操作。主库会阻止所有写入，一直到从库再次可用。

> 所以将所有从库都设置为同步是不切实际的。因为任何一个节点的中断都会影响到整个系统，这对高可用来说是致命的。
>
> 在现实使用场景，如果在数据库上启用同步复制，通常指其中**一个**从库是同步的，其他从库是异步的。如果该同步从库变得不可用或缓慢，则将一个异步从库改为同步运行。这保证你至少在两个节点上拥有最新的数据副本：主库和同步从库。 这种配置有时也被称为 **半同步（semi-synchronous）**。

通常情况下，基于领导者的复制都配置为完全异步。在这种情况下，如果主库失效且不可恢复，则任何尚未复制给从库的写入都会丢失。这意味着即使已经向客户端确认成功，写入也不能保证是 **持久（Durable）** 的。然而，一个完全异步的配置也有优点：即使所有的从库都落后了，主库也可以继续处理写入。



#### 设置新从库

有时候需要临时设置一个新的从库，可能是负载更大需要增加副本的数量，或者替换掉集群中长期失败的节点。

设置新从库会有如下流程：

1. 在某个时刻获取主库的一致性快照（如果可能，不必锁定整个数据库）。大多数数据库都具有这个功能，因为它是备份必需的。对于某些场景，可能需要第三方工具，例如用于 MySQL 的 innobackupex。
2. 将快照复制到新的从库节点。
3. 从库连接到主库，并拉取快照之后发生的所有数据变更。这要求快照与主库复制日志中的位置精确关联。该位置有不同的名称，例如 PostgreSQL 将其称为 **日志序列号（log sequence number，LSN）**，MySQL 将其称为 **二进制日志坐标（binlog coordinates）**。
4. 当从库处理完快照之后积累的数据变更，我们就说它 **赶上（caught up）** 了主库，现在它可以继续及时处理主库产生的数据变化了。

建立从库这个步骤需要根据不同的数据来进行配置，有些可能是自动化的，有些则需要管理手动操作。



#### 处理节点宕机

##### 从库失效：追赶恢复

在宿主机的本地磁盘中，会记录者从库收到的数据变更日志，如果从库崩溃并重新启动。从库可以从日志中得知，在发生故障之前处理的最后一个事务。因此，从库可以连接到主库，并请求在从库断开期间发生的所有数据变更。当应用完所有这些变更后，它就赶上了主库，并可以像以前一样继续接收数据变更流。

##### 主库失效：故障切换

主库失效处理起来相当棘手：其中一个从库需要被提升为新的主库，需要重新配置客户端，以将它们的写操作发送给新的主库，其他从库需要开始拉取来自新主库的数据变更。这个过程被称为 **故障切换（failover）**。

自动化切换：

1.确认主库失效。可以使用超时机制来确定主库是否挂了，类似于心跳包。

2.选举：剩余副本通过**共识算法**选举出一个新的主库。(也可以事先配置**控制节点**来指定新的主库)

3.重新配置系统以启用新的主库。如果旧主库恢复，让它成为一个从库。

> 故障自动切换过程中会相当麻烦且繁琐，不少运维团队更愿意手动执行故障切换。



#### 复制日志的实现

1.基于语句的复制

主库记录它执行的每个写入请求(语句)并将该语句日志发送给从库。每个从库解析并执行SQL语句，就像直接从客户端收到一样。

但是下列问题会让第一种实现变得数据不一致：

- 任何调用 **非确定性函数（nondeterministic）** 的语句，可能会在每个副本上生成不同的值。例如，使用 `NOW()` 获取当前日期时间，或使用 `RAND()` 获取一个随机数。
- 如果语句使用了 **自增列（auto increment）**，或者依赖于数据库中的现有数据（例如，`UPDATE ... WHERE <某些条件>`），则必须在每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制。
- 有副作用的语句（例如：触发器、存储过程、用户定义的函数）可能会在每个副本上产生不同的副作用，除非副作用是绝对确定性的。

> 通常情况下都不会选用这种复制方法



2.传输预写式日志（WAL）

- 对于日志结构存储引擎（请参阅 “[SSTables 和 LSM 树](https://github.com/Vonng/ddia/blob/master/ch3.md#SSTables和LSM树)”），日志是主要的存储位置。日志段在后台压缩，并进行垃圾回收。
- 对于覆写单个磁盘块的 [B 树](https://github.com/Vonng/ddia/blob/master/ch3.md#B树)，每次修改都会先写入 **预写式日志（Write Ahead Log, WAL）**，以便崩溃后索引可以恢复到一个一致的状态。

可以使用完全相同的日志在另一个节点上构建副本：除了将日志写入磁盘之外，主库还可以通过网络将其发送给从库。

通过使用这个日志，从库可以构建一个与主库一模一样的数据结构拷贝。

缺点：日志记录的数据非常底层，可能存在版本不兼容。会对运维团队造成麻烦。



3.逻辑日志复制(基于行)

对复制和存储引擎使用不同的日志格式，这样可以将复制日志从存储引擎的内部实现中解耦出来。这种复制日志被称为逻辑日志（logical log），以将其与存储引擎的（物理）数据表示区分开来。

关系数据库的逻辑日志通常是以行的粒度来描述对数据库表的写入记录的序列：

- 对于插入的行，日志包含所有列的新值。
- 对于删除的行，日志包含足够的信息来唯一标识被删除的行，这通常是主键，但如果表上没有主键，则需要记录所有列的旧值。
- 对于更新的行，日志包含足够的信息来唯一标识被更新的行，以及所有列的新值（或至少所有已更改的列的新值）。

> 修改多行的事务会生成多条这样的日志记录，后面跟着一条指明事务已经提交的记录。 MySQL 的二进制日志(当配置为使用基于行的复制时)使用了这种方法。



4.基于触发器的复制

有一些工具可以通过读取数据库日志，使其他应用程序可以使用数据。或者使用关系数据库自带的功能：触发器和存储过程。

触发器允许将数据更改(写入事务)发生时自动执行自定义应用程序代码注册早数据库系统中，触发器可以将更改记录到一个单独的表中，然后再使用外部应用程序读这个表，再加上一点自定义的逻辑就可以将数据复制到另一个系统中去。

基于触发器的复制通常比其他复制方法具有更高的开销，并且比数据库内置的复制更容易出错，也有很多限制。然而由于其灵活性，它仍然是很有用的。



#### 复制延迟问题

基于领导者的复制要求所有写入都由单个节点处理，但只读查询可以由任何一个副本来处理。所以对于读多写少的场景（Web 上的常见模式），一个有吸引力的选择是创建很多从库，并将读请求分散到所有的从库上去。这样能减小主库的负载，并允许由附近的副本来处理读请求。

在这种读伸缩（read-scaling）的体系结构中，只需添加更多的从库，就可以提高只读请求的服务容量。但是，这种方法实际上只适用于异步复制 —— 如果尝试同步复制到所有从库，则单个节点故障或网络中断将导致整个系统都无法写入。而且节点越多越有可能出现个别节点宕机的情况，所以完全同步的配置将是非常不可靠的。

但是异步会有一个问题，如果从库落后，会导致在某一个时间段存在数据不一致的问题。在正常操作中，**复制延迟**在实际中并不显眼，但是在极端情况下延迟会在几秒到几分钟钟不等。

这会造成三种情况：



1.读已之写

如果用户写入后马上查看数据，新数据可能没来得及到达副本，对于用户而言，看起来好像是刚提交的数据丢失了。

在这种情况下，我们需要 **写后读一致性（read-after-write consistency）**，也称为 **读己之写一致性（read-your-writes consistency）**。这是一个保证，如果用户重新加载页面，他们总会看到他们自己提交的任何更新。它不会对其他用户的写入做出承诺：其他用户的更新可能稍等才会看到。它保证用户自己的输入已被正确保存。

如何在基于领导者的复制系统中实现写后读一致性？文中提供了几种解决：

1.1对于**可能用户修改过**的内容，总是从主库读取。这就要求得有办法不通过实际的查询就可以知道用户是否修改了某些东西。例如：总是从主库读取用户自己的档案，如果要读取其他用户的档案就去从库。

1.2通过跟踪上次更新时间，在上次更新后的一分钟内从主库读。还可以监控从库的复制延迟，防止向任何滞后主库超过一分钟的从库发出查询。

1.3客户端记录最近一次写入的时间戳，系统需要确保从库在处理该用户的读取请求时，该时间戳前的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另一个从库读取，或者等待从库追赶上来。但是使用这个方法的时候要注意：**时钟同步变得至关重要**。

> 但是这种情况下记住用户上次更新时间戳的方法变得更加困难，因为一个设备上运行的程序不知道另一个设备上发生了什么。需要对这些元数据进行中心化的存储。



1.4如果副本分布在多个数据中心，还会变得更加复杂，因为任何需要主库提供服务的请求都必须路由到包含主库的数据中心。

> 如果副本分布在不同的数据中心，很难保证来自不同设备的连接会路由到同一数据中心。



2.单调读

如果先查询了一个延迟很小的从库，然后再查询一个延迟较大的从库。(数据可能也会存在差异，延迟较小的从库上存在，延迟较大的从库上可能不存在)这会让用户非常疑惑。

**单调读（monotonic reads）**可以保证这种异常不会发生。这是一个比 **强一致性（strong consistency）** 更弱，但比 **最终一致性（eventual consistency）** 更强的保证。当读取数据时，你可能会看到一个旧值；单调读仅意味着如果一个用户顺序地进行多次读取，则他们不会看到时间回退，也就是说，如果已经读取到较新的数据，后续的读取不会得到更旧的数据。

实现单调读的一种方式是确保每个用户总是从同一个副本进行读取（不同的用户可以从不同的副本读取）。例如，可以基于用户 ID 的散列来选择副本，而不是随机选择副本。但是，如果该副本出现故障，用户的查询将需要重新路由到另一个副本。

要防止某些分区的复制速度慢于其他分区造成的异常状态，需要另一种类型保证：**一致前缀读（consistent prefix reads）**，如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。

一种解决方案是，确保任何因果相关的写入都写入相同的分区，但在一些应用中可能无法高效地完成这种操作。



复制延迟最终解决方案：

- 在应用程序出做处理，但是容易出错且复杂。
- 使用分布式事务，但是不是所有数据库都支持。



### 多主复制

基于领导者的复制有一个主要的缺点：只有一个主库，而且所有的写入都必须通过它 [4](https://github.com/Vonng/ddia/blob/master/ch5.md#user-content-fn-iv-8006e2d5a73957a25f1adcd45244d11b)。如果出于任何原因（例如和主库之间的网络连接中断）无法连接到主库， 就无法向数据库写入。

基于领导者的复制模型的自然延伸是允许多个节点接受写入。复制仍然以同样的方式发生：处理写入的每个节点都必须将该数据变更转发给所有其他节点。我们将其称之为 **多领导者配置**（multi-leader configuration，也称多主、多活复制，即 master-master replication 或 active/active replication）。在这种情况下，每个主库同时是其他主库的从库。



#### 多主复制的应用场景

单个数据中心使用多个主库的配置没有什么意义，复杂性已经超过了能带来的好处。

假设你有一个数据库，副本分散在好几个不同的数据中心，如果使用常规的基于领导者的复制设置，主库必须位于其中一个数据中心，且所有写入都必须经过该数据中心。

但是如果使用多主配置，这样每个数据中心都有一个主库。在每个数据中心内使用常规的主从复制；在数据中心之间，每个数据中心的主库都会将其更改复制到其他数据中心的主库中。

多主配置的优点：

1.在多个数据中心的情况下，多主配置的性能可能要好于单主配置。

2.在多主配置中，每个数据中心可以独立于其他数据中心继续运行，并且当发生故障的数据中心归队时，复制会自动赶上。从这一点来说多主配置的可用性要高于单主配置。

3.采用异步复制功能的多主配置通常能更好地承受网络问题：临时的网络中断并不会妨碍正在处理的写入。

缺点：

1.**两个不同的数据中心可能会同时修改相同的数据，写冲突是必须解决的。**

2.由于多主复制在许多数据库中都属于改装的功能，所以常常存在微妙的配置缺陷，且经常与其他数据库功能之间出现意外的反应。比如自增主键、触发器、完整性约束等都可能会有麻烦。因此，多主复制往往被认为是危险的领域，应尽可能避免。



**多主复制的另一种适用场景是：应用程序在断网之后仍然需要继续工作。**

在这种情况下，每个设备都有一个充当主库的本地数据库（它接受写请求），并且在所有设备上的日历副本之间同步时，存在异步的多主复制过程。复制延迟可能是几小时甚至几天，具体取决于何时可以访问互联网。

从架构的角度来看，这种设置实际上与数据中心之间的多主复制类似，每个设备都是一个 “数据中心”，而它们之间的网络连接是极度不可靠的。从历史上各类日历同步功能的破烂实现可以看出，想把多主复制用好是多么困难的一件事。



**协同编辑场景下也适合**

当一个用户编辑文档时，所做的更改将立即应用到其本地副本（Web 浏览器或客户端应用程序中的文档状态），并异步复制到服务器和编辑同一文档的任何其他用户。

但是依旧要保证不会发生编辑冲突的问题，还需要应用文档的锁，然后用户才能对其编辑。如果另一个用户想要编辑同一个文档，他们首先必须等到第一个用户提交修改并释放锁定。这种协作模式相当于主从复制模型下在主节点上执行事务操作。



#### 处理写入冲突

多主复制的最大问题是可能发生写冲突，这意味着需要解决冲突。

原则上，可以使冲突检测同步 - 即等待写入被复制到所有副本，然后再告诉用户写入成功。但是，通过这样做，你将失去多主复制的主要优点：允许每个副本独立地接受写入。如果你想要同步冲突检测，那么你可能不如直接使用单主复制。

解决方案：

1.避免冲突，如果应用程序可以确保特定记录的所有写入都通过同一个主库，那么冲突就不会发生。例如，在一个用户可以编辑自己数据的应用程序中，可以确保来自特定用户的请求始终路由到同一数据中心，并使用该数据中心的主库进行读写。不同的用户可能有不同的 “主” 数据中心（可能根据用户的地理位置选择），但从任何一位用户的角度来看，本质上就是单主配置了。

> 但是也会出现另一种情况：因为某个数据中心出现故障，需要将流量重新路由到另一个数据中心，在这种情况下，无法避免冲突。必须处理不同主库同时写入的可能性。

2.收敛至一致的状态，这意味着所有副本必须在所有变更复制完成时**收敛**至一个相同的最终值。

- 给每个写入一个唯一的 ID（例如时间戳、长随机数、UUID 或者键和值的哈希），挑选最高 ID 的写入作为胜利者，并丢弃其他写入。如果使用时间戳，这种技术被称为 **最后写入胜利（LWW, last write wins）**。虽然这种方法很流行，但是很容易造成数据丢失。
- 为每个副本分配一个唯一的 ID，ID 编号更高的写入具有更高的优先级。这种方法也意味着数据丢失。
- 以某种方式将这些值合并在一起 - 例如，按字母顺序排序，然后连接它们。
- 用一种可保留所有信息的显式数据结构来记录冲突，并编写解决冲突的应用程序代码（也许通过提示用户的方式，类似Git？）。



解决冲突的最合适的方法可能取决于应用程序：

写时执行：只要数据库检测到复制更改日志中存在冲突，就调用冲突处理程序。

读时执行：当检测到冲突时，所有冲突写入被存储。下一次读取数据时，会将这些多个版本的数据返回给应用程序。应用程序可以提示用户或自动解决冲突，并将结果写回数据库。

> 自动冲突解决会随着时间推移变得越来越复杂，自定义的代码也可能出错。



#### 多主复制拓扑

**复制拓扑**（replication topology）用来描述写入操作从一个节点传播到另一个节点的通信路径。比如主库1将所有的写入发送到主库2中。

常见的三种拓扑：

1.全部到全部(all-to-all)

环形拓扑，其中每一个节点都从一个节点接受写入，并将这些写入(及自己的写入)全都转发给另外一个节点。

星型形状，一个指定的根节点将写入转发给所有其他节点。星型拓扑可以推广到树。

在环形和星形拓扑中，写入可能需要在到达所有副本之前通过多个节点。因此，节点需要转发从其他节点收到的数据更改。为了防止无限复制循环，每个节点被赋予一个唯一的标识符，并且在复制日志中，每次写入都会使用其经过的所有节点的标识符进行标记。当一个节点收到用自己的标识符标记的数据更改时，该数据更改将被忽略，因为节点知道它已经被处理过。

> 如果只有一个节点发生故障，则可能会中断其他节点之间的复制消息流，导致它们无法通信，除非节点被修复。
>
> 也可以重新配置为跳过发生故障的节点，但是大多数部署中，这种操作必须手动完成。

更密集连接的拓扑结构（例如全部到全部）的容错性更好，因为它允许消息沿着不同的路径传播，可以避免单点故障。

可能出现的问题：

多主复制时使用All-to-all 可能会因为网络拥塞，不同主库可能**写入顺序不一致**。更新取决于先前的插入，所以我们需要确保所有节点先处理插入，然后再处理更新。

要正确的排序这些时间，可以使用**版本向量(**version vectors)技术

> 注意：很多多主复制系统中的冲突检测技术实现的并不好，如果你正在使用基于多主复制的系统，那么你应该多了解这些问题，仔细阅读文档，并彻底测试你的数据库，以确保它确实提供了你想要的保证。



### 无主复制

一些数据存储系统采用不同的方法，放弃主库的概念，允许任何副本直接接受来自客户端的写入。最早的一些的复制数据系统是 **无主的（leaderless）**。

在一些无主复制的实现中，客户端直接将写入发送到几个副本中，而另一些情况下，由一个 **协调者（coordinator）** 节点代表客户端进行写入。但与主库数据库不同，协调者不执行特定的写入顺序。我们将会看到，这种设计上的差异对数据库的使用方式有着深远的影响。



#### 当节点故障时写入数据库

如果是基于领导者的配置中，从库中某个副本不可用时，要继续处理写入，则可能需要执行故障切换。

但是在无主配置中，不存在故障转移这种情况。某个副本在不接受写入时，会直接忽略**错过写入的事实。**这种情况下用户从这个副本上拿取数据可能会拿到过时的值。

所以为了解决这个问题，当一个客户端从数据库中读取数据时，它不仅仅把它的请求发送到一个副本：读请求将被并行地发送到多个节点。客户可能会从不同的节点获得不同的响应，即来自一个节点的最新值和来自另一个节点的陈旧值。版本号将被用于确定哪个值是更新的。

两种机制保证不可用节点恢复后，使它的数据与最新的数据保持一致：

- 读修复（Read repair）：当客户端并行读取多个节点时，它可以检测到任何陈旧的响应。如果发现具有陈旧值，将新值写回到该副本。这种方法适用于读频繁的值。
- 反熵过程（Anti-entropy process）：此外，一些数据存储具有后台进程，该进程不断查找副本之间的数据差异，并将任何缺少的数据从一个副本复制到另一个副本。与基于领导者的复制中的复制日志不同，此反熵过程不会以任何特定的顺序复制写入，并且在复制数据之前可能会有显著的延迟。

> 如果没有反熵过程，很少被读取的值可能会从某些副本中丢失，从而降低了持久性，因为只有在应用程序读取值时才执行读修复。



在Dynamo风格的数据库中，需要配置副本数量(n个)以及**每个写入必须由 w 个节点确认**才能被认为是成功的，并且至少为每个读取查询 r 个节点。通常情况下这样设计是为了数学中的公式推理。

![image-20230817104712279](https://raw.githubusercontent.com/AnAnonymousFriend/images/main/image-20230817104712279.png)

> 尽管法定人数似乎保证读取返回最新的写入值，但在实践中并不那么简单。 Dynamo 风格的数据库通常针对可以忍受最终一致性的用例进行优化。你可以通过参数 w 和 r 来调整读取到陈旧值的概率，但把它们当成绝对的保证是不明智的。



无主复制的系统中，没有固定的写入顺序会让监控变得更加困难。而且，如果数据库只使用读修复（没有反熵过程），那么对于一个值可能会有多陈旧其实是没有限制的 - 如果一个值很少被读取，那么由一个陈旧副本返回的值可能是古老的(甚至几乎是废弃的)。

数据库冲突会出现以下几种情况：

**最后写入胜利(丢弃并发写入)**

实现最终收敛的一种方法是声明每个副本只需要存储 **“最近”** 的值，并允许 **“更旧”** 的值被覆盖和抛弃。然后，只要我们有一种明确的方式来确定哪个写是 “最近的”，并且每个写入最终都被复制到每个副本，那么复制最终会收敛到相同的值。

可以为每个写入附加一个时间戳，然后挑选最大的时间戳作为 **“最近的”**，并丢弃具有较早时间戳的任何写入。这种冲突解决算法被称为 **最后写入胜利（LWW, last write wins）**。

LWW 实现了最终收敛的目标，但以 **持久性** 为代价：如果同一个键有多个并发写入，即使它们反馈给客户端的结果都是成功的（因为它们被写入 w 个副本），也只有一个写入将被保留，而其他写入将被默默地丢弃。

> 在类似缓存的一些情况下，写入丢失可能是可以接受的。但如果数据丢失不可接受，LWW 是解决冲突的一个很烂的选择。
>
> **在数据库中使用 LWW 的唯一安全方法是确保一个键只写入一次，然后视为不可变，从而避免对同一个键进行并发更新。**



**什么是并发性？**

如果两个操作“同时”发生，似乎应该成为并发。但是实际上由于分布式系统中的时钟问题，很难判断两个事件是否是**同时**发生的。

如果两个操作都意识不到对方的存在，就成为这两个操作**并发**。

在计算机系统中，即使光速原则上允许一个操作影响另一个操作，但两个操作也可能是 **并发的**。例如，如果网络缓慢或中断，两个操作间可能会出现一段时间间隔，但仍然是并发的，因为网络问题阻止一个操作意识到另一个操作的存在。



### 本章小结

复制的目的：

高可用性，无论是数据中心，还是服务器宕机时，都能保持系统正常运行;允许程序在网络中断时继续工作;将数据存放在离用户相对较近的地理位置;可伸缩性，通过副本读的方式，可以处理比单机更大的读取量。

复制的三种主要方法：

单主复制：客户端将所有写入操作发送到单个节点（主库），该节点将数据更改事件流发送到其他副本（从库）。读取可以在任何副本上执行，但从库的读取结果可能是陈旧的。

多主复制：客户端将每个写入发送到几个主库节点之一，其中任何一个主库都可以接受写入。主库将数据更改事件流发送给彼此以及任何从库节点。

无主复制：客户端将每个写入发送到几个节点，并从多个节点并行读取，以检测和纠正具有陈旧数据的节点。

上述复制方法都有不同的优点及缺点，在不同场景下要考虑好使用那种复制方法。

由于复制延迟引起的效应，文中也记录了一些应用程序在复制延迟时的行为一致性模型。

写后读一致性：用户应该总是能看到自己提交的数据。

单调读：用户在看到某个时间点的数据后，不应该看到这个数据在时间点前的情况。

一致前缀读：用户应该看到数据处于一种具有因果意义的状态：例如，按正确的顺序看到一个问题和对应的回答。

最后本章还讲述了多主复制和无主复制方法遇到的并发问题，还记录了通过合并并发更新来解决冲突。





## 分区

对于非常大的数据集，或非常高的吞吐量，我们需要将数据**分区（partitions）**，也称为 **分片（sharding）**。

每个分区都是自己的小型数据库，尽管数据库可能支持同时进行多个分区的操作。

分区主要是为了**可伸缩性**，不同的分区可以放在不共享集群中的不同节点上。因此，大数据集可以分布在多个磁盘上，并且查询负载可以分布在多个处理器上。

对于在单个分区上运行的查询，每个节点可以独立执行对自己的查询，因此可以通过添加更多的节点来扩大查询吞吐量。大型，复杂的查询可能会跨越多个节点并行处理，尽管这也带来了新的困难。



### 分区和复制

分区通常和复制结合使用，使每个分区的副本存储在多个节点上。这意味着，即使每条记录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。

一个节点可能存储多个分区。如果使用主从复制模型则每个分区的主库被分配给一个节点，从库被分配给其他节点。所以每个节点可能是某些分区的主库，同时是其他分区的从库。

大多数情况下，分区方案的选择与复制方案的选择是独立的。



### 键值数据的分区

分区的目的是将数据和查询负载均匀分布在各个节点上，理论上如果每个节点公平分享数据和负载，那么处理的数据量和读写吞吐量应该是呈N倍(节点数量)增长。

但是如果分区不是公平的，一些分区有比其他分区更多的数据或查询，这被称为**偏斜**。数据偏斜会导致分区效率下降很多，极端情况下所有的负载可能压在一个分区上，瓶颈落在一个节点上。不均衡的高负载分区被称为**热点**。

避免热点最简单的方法是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点。

Key Range 分区的缺点是某些特定的访问模式会导致热点。 如果主键是时间戳，则分区对应于时间范围，例如，给每天分配一个分区。 不幸的是，由于我们在测量发生时将数据从传感器写入数据库，因此所有写入操作都会转到同一个分区（即今天的分区），这样分区可能会因写入而过载，而其他分区则处于空闲状态。

为了避免传感器数据库中的这个问题，需要使用除了时间戳以外的其他东西作为主键的第一个部分。 例如，可以在每个时间戳前添加传感器名称，这样会首先按传感器名称，然后按时间进行分区。 假设有多个传感器同时运行，写入负载将最终均匀分布在不同分区上。 现在，当想要在一个时间范围内获取多个传感器的值时，你需要为每个传感器名称执行一个单独的范围查询。



### 根据键的散列分区

很多分布式数据存储使用散列函数来确定给定键的分区。

一个好的散列函数可以将偏斜的数据均匀分布。假设你有一个 32 位散列函数，无论何时给定一个新的字符串输入，它将返回一个 0 到 232 -1 之间的 “随机” 数。即使输入的字符串非常相似，它们的散列也会均匀分布在这个数字范围内。

**按哈希键分区**

这种技术擅长在分区之间公平地分配键。分区边界可以是均匀间隔的，也可以是伪随机选择的（在这种情况下，该技术有时也被称为 **一致性哈希**，即 consistent hashing）。

> 不幸的是，通过使用键散列进行分区，我们失去了键范围分区的一个很好的属性：高效执行范围查询的能力。曾经相邻的键现在分散在所有分区中，所以它们之间的顺序就丢失了。

但是有一种折衷的策略，可以由多个列组成的复合主键来声明。键中只有第一列会作为散列的依据，而其他列则被用作SSTables 中排序数据的连接索引。尽管查询无法在复合主键的第一列中按范围扫表，但如果第一列已经指定了固定值，则可以对该键的其他列执行有效的范围扫描。

组合索引方法为一对多关系提供了一个优雅的数据模型。可以有效地检索特定用户在某个时间间隔内按时间戳排序的所有更新。不同的用户可以存储在不同的分区上，对于每个用户，更新按时间戳顺序存储在单个分区上。



### 负载偏斜与热点消除

哈希分区可以帮助减少热点。但是，它不能完全避免它们：在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区。

大多数数据系统都无法自动补偿这种高度偏斜的负载，因此将减少偏斜的责任放在应用程序上。例如，如果一个主键被认为是非常火爆的，一个简单的方法是在主键的开始或结尾添加一个随机数。只要一个两位数的十进制随机数就可以将主键分散为 100 种不同的主键，从而存储在不同的分区中。

主键分割以后，任何读取都必须要做额外的工作，因为他们必须从所有 100 个主键分布中读取数据并将其合并。此技术还需要额外的记录：只需要对少量热点附加随机数；对于写入吞吐量低的绝大多数主键来说是不必要的开销。因此，你还需要一些方法来跟踪哪些键需要被分割。



### 分区与次级索引

上文中内容都依赖于键值数据模型。如果只通过主键访问记录，我们可以从该键确定分区，并使用它来将读写请求路由到负责该键的分区。

如果涉及次级索引，情况就会变得更加复杂。次级索引通常并不能唯一地标识记录，而是一种搜索记录中出现特定值的方法：查找用户的所有操作，查找包含某个关键词的文章等等。

次级索引是关系性数据库等基础，并且在文档数据库中也很普遍。许多键值存储为了减少实现的复杂度而放弃了次级索引，但是一些（如 Riak）已经开始添加它们，因为它们对于数据模型实在是太有用了。并且次级索引也是 Solr 和 Elasticsearch 等搜索服务器的基石。

**次级索引的问题是它们不能整齐地映射到分区。**有两种用次级索引对数据库进行分区的方法：



#### 基于文档的次级索引进行分区

在这种索引方法中，每个分区是完全独立的：每个分区维护自己的次级索引，仅覆盖该分区中的文档。它不关心存储在其他分区的数据。无论何时你需要写入数据库（添加，删除或更新文档），只需处理包含你正在编写的文档 ID 的分区即可。出于这个原因，**文档分区索引** 也被称为 **本地索引**。

> 注意：除非对文档ID做了特别的处理，否则不能按照某一特性将对象放在同一个分区中。

这种查询分区数据库的方法有时被称为 **分散 / 聚集（scatter/gather）**，并且可能会使次级索引上的读取查询相当昂贵。即使并行查询分区，分散 / 聚集也容易导致尾部延迟放大。



#### 基于关键词(Term)的次级索引进行分区

我们可以构建一个覆盖所有分区数据的 **全局索引**，而不是给每个分区创建自己的次级索引（本地索引）。但是，我们不能只把这个索引存储在一个节点上，因为它可能会成为瓶颈，违背了分区的目的。全局索引也必须进行分区，但可以采用与主键不同的分区方式。

我们将这种索引称为 **关键词分区（term-partitioned）**，因为我们寻找的关键词决定了索引的分区方式。例如，一个关键词可能是：`color:red`。**关键词（Term）** 这个名称来源于全文搜索索引（一种特殊的次级索引），指文档中出现的所有单词。

通过**关键词**本身或它的散列进行索引分区。根据关键词本身来分区对于范围扫描非常有用，而关键词的哈希分区提供了负载均衡的能力。

关键词分区的全局索引**优于**文档分区索引的地方点是它可以使读取更有效率：不需要 **分散 / 收集** 所有分区，客户端只需要向包含关键词的分区发出请求。

全局索引的**缺点**在于写入速度较慢且较为复杂，因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分区或者不同的节点上） 。

> 注意，在关键词分区索引中，需要跨分区的分布式事务，并不是所有数据库都支持。

实际中，全局次级索引的更新通常是**异步**的，如果在写入之后不久读取索引，刚才所做的更改可能尚未反映在索引中。



### 分区再平衡

随着时间的推移，数据库会有各种变化，比如查询吞吐量增加，数据集大小增加，机器出故障。为了应用这些变化，所以我们需要添加更多的CPU来处理吞吐量;增加更多的磁盘和RAM来存储;需要其他的机器接管故障的机器。

这些更改都需要数据和请求从一个节点移动到另一个节点。负载从集群中的一个节点向另一个节点移动的过程称为 **再平衡**。

无论使用哪种分区方案，再平衡通常都要满足一些最低要求：

- 再平衡之后，负载（数据存储，读取和写入请求）应该在集群中的节点之间公平地共享。
- 再平衡发生时，数据库应该继续接受读取和写入。
- 节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘 I/O 负载。



#### 再平衡策略

1.固定数量分区：创建比节点更多的分区，并为每个节点分配多个分区。如果一个新节点添加到集群中，新节点可以从当前每一个节点中**窃取**一些分区，直至分区再次公平分配。

只有分区在节点之间移动。分区数量不会改变，所指定的分区也不会改变。唯一改变的是分区所在的节点。这种变更并不是即时的 — 在网络上传输大量的数据需要一些时间 — 所以在传输过程中，原有分区仍然会接受读写操作。

原则上，你甚至可以解决集群中的硬件不匹配问题：通过为更强大的节点分配更多的分区，可以强制这些节点承载更多的负载。

许多固定分区数据库选择不实施分区分割。因此，一开始配置的分区数就是你可以拥有的最大节点数量，所以你需要选择足够多的分区以适应未来的增长。但是，每个分区也有管理开销，所以选择太大的数字会适得其反。

> 如果数据的大小难以估量，选择正确的分区是很困难的。如果分区非常大，再平衡和从节点故障恢复变得昂贵。但是，如果分区太小，则会产生太多的开销。当分区大小 “恰到好处” 的时候才能获得很好的性能，如果分区数量固定，但数据量变动很大，则难以达到最佳性能。



2.动态分区：

按键的范围进行分区的数据库会动态创建分区。当分区增长到超过配置的大小时，会被分成两个分区,那么每个分区约占一半的数据。相对应的，如果数据太小缩至某个阈值以下，则可以将其与相邻分区合并。

每个分区分配给一个节点，每个节点可以处理多个分区，就像固定数量的分区一样。大型分区拆分后，可以将其中的一半转移到另一个节点，以平衡负载。

优点：动态分区会让分区数量适应总数据量。如果只有少量的数据，少量的分区就足够了，开销也会变得更少；如果有大量的数据，每个分区的大小被限制在一个可配置的最大值。

一些数据库会使用**预分割**初始化分区，在这种情况下要提前预知如何进行分配。

动态分区不仅适用于数据的范围分区，而且也适用于散列分区。



3.按节点比例分区

通过动态分区，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在固定的最小值和最大值之间。另一方面，对于固定数量的分区，每个分区的大小与数据集的大小成正比。在这两种情况下，分区的数量都与节点的数量无关。

分区数与节点数成正比：每个节点具有固定数量的分区。在这种情况下，每个分区的大小与数据集大小成比例地增长，而节点数量保持不变，但是当增加节点数时，分区将再次变小。由于较大的数据量通常需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定。

当一个新节点加入集群时，随机选择固定数量的现有分区进行拆分，然后占有这些拆分分区中每个分区的一半，同时将每个分区的另一半留在原地。随机化可能会产生不公平的分割，但是平均在更大数量的分区上时，新节点最终从现有节点获得公平的负载份额。

随机选择分区边界要求使用基于散列的分区。



### 请求路由

**服务发现**可以解决客户端请求时需要访问哪个IP地址和端口的问题。

一般有几种方案：

1.允许客户端访问所有节点。（例如，通过 **循环策略的负载均衡**，即 Round-Robin Load Balancer）。如果访问的节点恰好有请求的分区，则直接处理请求。如果没有则转发到存在分区的节点上，接收回复并传递给客户端。

2.通过一个路由层处理请求的节点，并相应转发。此路由层本身不处理任何请求；它仅负责分区的负载均衡。

3.客户端知道分区和节点的分配策略，客户端做逻辑判断，直接访问节点。



许多分布式数据系统都依赖于一个独立的协调服务，比如每个节点在中间件服务中注册自己，然后依靠中间件维护分区到节点的可靠映射。



### 本章小结

分区这章内容较多，但是主要了解将大数据集划分成更小的子集的不同方法。

分区的目标是在多台机器上均匀分布数据和查询负载，避免出现热点（负载不成比例的节点）。

文中记录了两种分区方法：

1.**键范围分区**：通过有序的键，分区拥有从某个最小值到某个最大值的所有键。排序可以进行有效的范围查询,但是如果应用程序经常访问相邻的键，则存在热点的风险。在这种方法中，当分区变得太大时，通常将分区分成两个子分区来动态地重新平衡分区。

2.**散列分区**:散列函数应用于每个键，分区拥有一定范围的散列。这会让查询效率变低，但是可以更均匀地分配负载。通过散列进行分区时，通常先提前创建固定数量的分区，为每个节点分配多个分区，并在添加或删除节点时将整个分区从一个节点移动到另一个节点。也可以使用动态分区。

> 当然，使用复合主键也行：使用键的一部分来标识分区，而使用另一部分作为排序顺序。

还讨论了分区和次级索引之间的相互作用。次级索引也需要分区，有两种方法：

- 基于文档分区（本地索引），其中次级索引存储在与主键和值相同的分区中。这意味着只有一个分区需要在写入时更新，但是读取次级索引需要在所有分区之间进行分散 / 收集。
- 基于关键词分区（全局索引），其中次级索引存在不同的分区中。次级索引中的条目可以包括来自主键的所有分区的记录。当文档写入时，需要更新多个分区中的次级索引；但是可以从单个分区中进行读取。

最后讨论了通过路由将查询分配到分区的技术。
