---
title: 设计数据密集型应用——数据系统基础
date: 2023-08-01 10:52:00
tags: [分布式,学习笔记]
category: 分布式
---

## 可靠性,可伸缩性和可维护性

如今大部分的应用程序都是**数据密集**，而非**计算密集形**，所以CPU很少成为这类应用的瓶颈，更大的问题通常来自数据量，数据复杂性，以及数据的变更速度。

大部分程序都需要以下功能：

- 存储数据，以便自己或其他应用程序之后能再次找到 （*数据库，即 databases*）

- 记住开销昂贵操作的结果，加快读取速度（*缓存，即 caches*）

- 允许用户按关键字搜索数据，或以各种方式对数据进行过滤（*搜索索引，即 search indexes*）

- 向其他进程发送消息，进行异步处理（*流处理，即 stream processing*）

- 定期处理累积的大批量数据（*批处理，即 batch processing*）

  

  下文将简述不用工具之间

不同的应用有着不同的需求，所以数据库系统也是有着各式各样的特性，例如常用的Mysql（关系型数据库）和MongoDB(非关系性数据库)。实现缓存的也有各种不同的手段。当单个工具解决不了当前问题时，有时需要组合使用这些工具。下文将探索不同工具之间的共性与特性，以及各自实现原理。

本章将从我们所要实现的基础目标开始：可靠、可伸缩、可维护的数据系统。我们将澄清这些词语的含义，概述考量这些目标的方法。并回顾一些后续章节所需的基础知识。在接下来的章节中我们将抽丝剥茧，研究设计数据密集型应用时可能遇到的设计决策。



### 关于数据库的思考

我们通常认为，数据库，消息队列，缓存等工具分属几个差异明显的类别。虽然数据库和消息队列表面上存在一些相似性，比如：它们都会存储一段时间的数据——但是它们有迥然不同的访问模式，这意味着迥异的性能特征和实现手段。

那我们为什么要把这些东西放在 **数据系统（data system）** 的总称之下混为一谈呢？

近些年来，出现了许多新的数据存储工具与数据处理工具。它们针对不同应用场景进行优化，因此不再适合生硬地归入传统类别【1】。类别之间的界限变得越来越模糊，例如：数据存储可以被当成消息队列用（Redis），消息队列则带有类似数据库的持久保证（Apache Kafka）。

其次，越来越多的应用程序有着各种严格而广泛的要求，单个工具不足以满足所有的数据处理和存储需求。取而代之的是，总体工作被拆分成一系列能被单个工具高效完成的任务，并通过应用代码将它们缝合起来。

#### 一个可能的组合使用多个组件的数据系统架构

当多个工具组合在一起提供服务时，服务的接口或者**应用程序编程接口**通常向客户端隐藏这些实现细节。

设计数据系统或服务时，可能会遇到很棘手的问题：当系统出问题时，如何确保数据的正确性和完整性？当部分系统退化降级时，如何为客户提供始终如一的良好性能？当负载增加时，如何扩容应对？

影响数据系统设计的因素很多，包括参与人员的技能和经验、历史遗留问题、系统路径依赖、交付时限、公司的风险容忍度、监管约束等，这些因素都需要具体问题具体分析。

本书着重讨论三个在大多数软件系统中都很重要的问题：

- 可靠性（Reliability）

  系统在 **困境**（adversity，比如硬件故障、软件故障、人为错误）中仍可正常工作（正确完成功能，并能达到期望的性能水准）。请参阅 “[可靠性](https://github.com/Vonng/ddia/blob/master/ch1.md#可靠性)”。

- 可伸缩性（Scalability）

  有合理的办法应对系统的增长（数据量、流量、复杂性）。请参阅 “[可伸缩性](https://github.com/Vonng/ddia/blob/master/ch1.md#可伸缩性)”。

- 可维护性（Maintainability）

  许多不同的人（工程师、运维）在不同的生命周期，都能高效地在系统上工作（使系统保持现有行为，并适应新的应用场景）。请参阅 “[可维护性](https://github.com/Vonng/ddia/blob/master/ch1.md#可维护性)”。

人们经常追求这些词汇，却没有清楚理解它们到底意味着什么。为了工程的严谨性，本章的剩余部分将探讨可靠性、可伸缩性和可维护性的含义。为实现这些目标而使用的各种技术，架构和算法将在后续的章节中研究。

### 可靠性

人们对于一个东西是否可靠，都有一个直观的想法。人们对可靠软件的典型期望包括：

- 应用程序表现出用户所期望的功能。
- 允许用户犯错，允许用户以出乎意料的方式使用软件。
- 在预期的负载和数据量下，性能满足要求。
- 系统能防止未经授权的访问和滥用。

如果所有这些在一起意味着“正确工作”,那么可以把可靠性粗略理解为：“即使出现问题，也能继续正常工作”。

造成错误的原因叫做 **故障（fault）**，能预料并应对故障的系统特性可称为 **容错（fault-tolerant）** 或 **韧性（resilient）**。“**容错**” 一词可能会产生误导，因为它暗示着系统可以容忍所有可能的错误，但在实际中这是不可能的。

**故障（fault）** 不同于 **失效（failure）**。**故障** 通常定义为系统的一部分状态偏离其标准，而 **失效** 则是系统作为一个整体停止向用户提供服务。故障的概率不可能降到零，因此最好设计容错机制以防因 **故障** 而导致 **失效**。本书中我们将介绍几种用不可靠的部件构建可靠系统的技术。

反直觉的是，在这类容错系统中，通过故意触发来 **提高** 故障率是有意义的，例如：在没有警告的情况下随机地杀死单个进程。许多高危漏洞实际上是由糟糕的错误处理导致的，因此我们可以通过故意引发故障来确保容错机制不断运行并接受考验，从而提高故障自然发生时系统能正确处理的信心。

但本书主要讨论的是可以恢复的故障种类，正如下面几节所述。

### 硬件故障

当想到系统失效的原因时，**硬件故障（hardware faults）** 总会第一个进入脑海。硬盘崩溃、内存出错、机房断电、有人拔错网线…… 任何与大型数据中心打过交道的人都会告诉你：一旦你拥有很多机器，这些事情 **总** 会发生！

据报道称，硬盘的 **平均无故障时间（MTTF, mean time to failure）** 约为 10 到 50 年。因此从数学期望上讲，在拥有 10000 个磁盘的存储集群上，平均每天会有 1 个磁盘出故障。

为了减少系统的故障率，第一反应通常都是增加单个硬件的冗余度，例如：磁盘可以组建 RAID，服务器可能有双路电源和热插拔 CPU，数据中心可能有电池和柴油发电机作为后备电源，某个组件挂掉时冗余组件可以立刻接管。这种方法虽然不能完全防止由硬件问题导致的系统失效，但它简单易懂，通常也足以让机器不间断运行很多年。

直到最近，硬件冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕见。只要你能快速地把备份恢复到新机器上，故障停机时间对大多数应用而言都算不上灾难性的。只有少量高可用性至关重要的应用才会要求有多套硬件冗余。

但是随着数据量和应用计算需求的增加，越来越多的应用开始大量使用机器，这会相应地增加硬件故障率。此外，在类似亚马逊 AWS（Amazon Web Services）的一些云服务平台上，虚拟机实例不可用却没有任何警告也是很常见的，因为云平台的设计就是优先考虑 **灵活性（flexibility）** 和 **弹性（elasticity）**[1](https://github.com/Vonng/ddia/blob/master/ch1.md#user-content-fn-i-aa07792357b4b3eb1eccfe7d1ddf838f)，而不是单机可靠性。

如果在硬件冗余的基础上进一步引入软件容错机制，那么系统在容忍整个（单台）机器故障的道路上就更进一步了。这样的系统也有运维上的便利，例如：如果需要重启机器（例如应用操作系统安全补丁），单服务器系统就需要计划停机。而允许机器失效的系统则可以一次修复一个节点，无需整个系统停机。

> 应对硬件故障的方式，笼统一点就是：增加硬件的冗余度，在某个组件挂掉时冗余组件可以立刻接管，这种方式对于大多数应用来说已经足够了。

### 软件错误

如果说硬件故障时随机性的，独立的：一台机器的磁盘失效不意味另一台机器的磁盘也会失效。虽然大量硬件组件之间可能存在微弱的相关性（例如服务器机架的温度等共同的原因），但同时发生故障也是极为罕见的。

> 但是实际情况……可能因为没做异地容灾出现集体鼓掌挺常见的，比如园区停电，物理机中毒等原因导致虚拟机集体崩溃。

另一类错误是内部的 **系统性错误（systematic error）**。这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬件故障往往可能造成更多的 **系统失效**。例子包括：

- 接受特定的错误输入，便导致所有应用服务器实例崩溃的 BUG。例如 2012 年 6 月 30 日的闰秒，由于 Linux 内核中的一个错误【9】，许多应用同时挂掉了。
- 失控进程会用尽一些共享资源，包括 CPU 时间、内存、磁盘空间或网络带宽。
- 系统依赖的服务变慢，没有响应，或者开始返回错误的响应。
- 级联故障，一个组件中的小故障触发另一个组件中的故障，进而触发更多的故障【10】。

导致这类软件故障的 BUG 通常会潜伏很长时间，直到被异常情况触发为止。这种情况意味着软件对其环境做出了某种假设 —— 虽然这种假设通常来说是正确的，但由于某种原因最后不再成立了。

虽然软件中的系统性故障没有速效药，但我们还是有很多小办法，例如：仔细考虑系统中的假设和交互；彻底的测试；进程隔离；允许进程崩溃并重启；测量、监控并分析生产环境中的系统行为。如果系统能够提供一些保证（例如在一个消息队列中，进入与发出的消息数量相等），那么系统就可以在运行时不断自检，并在出现 **差异（discrepancy）** 时报警。